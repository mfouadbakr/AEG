{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52f5M9GH1uLz"
      },
      "outputs": [],
      "source": [
        "# --- Cell 1: Environment Setup ---\n",
        "print(\"‚öôÔ∏è Installing Dependencies (This takes ~5-8 mins for vLLM)...\")\n",
        "# Uninstall standard torch to allow vLLM to install its specific version if needed\n",
        "!pip uninstall -y torch\n",
        "!pip install -q vllm\n",
        "!pip install -q -U transformers accelerate sentence-transformers faiss-cpu json_repair textstat\n",
        "!pip install -q pandas numpy scikit-learn matplotlib seaborn\n",
        "\n",
        "# Re-import essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import json\n",
        "import textstat\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import cohen_kappa_score, mean_absolute_error\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from json_repair import repair_json\n",
        "from tqdm.auto import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Check GPU\n",
        "print(f\"‚úÖ Environment Ready.\")\n",
        "!nvidia-smi\n",
        "\n",
        "# --- Cell 2: SMART CONTROL PANEL ---\n",
        "\n",
        "# 1. USER SELECTION\n",
        "SELECTED_ESSAY_SET = 6  # <--- CHANGE THIS (1-8)\n",
        "SAMPLE_SIZE = None      # Set to None for FULL RUN\n",
        "\n",
        "# 2. CONFIG DATABASE\n",
        "ESSAY_CONFIGS = {\n",
        "    1: {'ctx': 3000, 'tok': 200, 'batch': 8,  'type': 'Persuasive'},\n",
        "    2: {'ctx': 3000, 'tok': 200, 'batch': 8,  'type': 'Persuasive'},\n",
        "    3: {'ctx': 1500, 'tok': 150, 'batch': 16, 'type': 'Source-Based'},\n",
        "    4: {'ctx': 1500, 'tok': 150, 'batch': 16, 'type': 'Source-Based'},\n",
        "    5: {'ctx': 1500, 'tok': 150, 'batch': 16, 'type': 'Source-Based'},\n",
        "    6: {'ctx': 1500, 'tok': 150, 'batch': 16, 'type': 'Source-Based'},\n",
        "    7: {'ctx': 2000, 'tok': 200, 'batch': 10, 'type': 'Narrative'},\n",
        "    8: {'ctx': 6000, 'tok': 300, 'batch': 2,  'type': 'Narrative'}\n",
        "}\n",
        "\n",
        "# 3. AUTO-CONFIGURE\n",
        "cfg = ESSAY_CONFIGS[SELECTED_ESSAY_SET]\n",
        "print(f\"üîß CONFIGURING FOR SET {SELECTED_ESSAY_SET} ({cfg['type']})\")\n",
        "print(f\"   -> Context: {cfg['ctx']} | Batch: {cfg['batch']} | Tokens: {cfg['tok']}\")\n",
        "\n",
        "CONTROL_PANEL = {\n",
        "    'essay_set': SELECTED_ESSAY_SET,\n",
        "    'target_col': 'domain1_score',\n",
        "    'pilot_sample_size': SAMPLE_SIZE,\n",
        "    'random_state': 42,\n",
        "    'checkpoint_path': f\"thesis_checkpoint_set{SELECTED_ESSAY_SET}.csv\",\n",
        "\n",
        "    # vLLM Settings (Dynamic)\n",
        "    'llm_model': \"casperhansen/deepseek-r1-distill-llama-8b-awq\",\n",
        "    'max_new_tokens': cfg['tok'],\n",
        "    'context_char_limit': cfg['ctx'],\n",
        "    'vllm_batch_size': cfg['batch'],\n",
        "    'gpu_memory_utilization': 0.7,\n",
        "\n",
        "    # RAG Settings\n",
        "    'embedding_model': 'all-MiniLM-L6-v2',\n",
        "    'top_k_numeric': 5\n",
        "}\n",
        "\n",
        "# --- Cell 3: Data Loading ---\n",
        "def load_and_prep_data(path=None):\n",
        "    if path is None:\n",
        "        print(\"üìÇ Upload your dataset (ASAP 'training_set_rel3.tsv' or .xlsx)...\")\n",
        "        uploaded = files.upload()\n",
        "        fn = next(iter(uploaded))\n",
        "        df = pd.read_excel(fn) if fn.endswith('.xlsx') else pd.read_csv(fn, encoding='latin-1', sep='\\t' if fn.endswith('.tsv') else ',')\n",
        "    else:\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "    df = df[df['essay_set'] == CONTROL_PANEL['essay_set']].copy()\n",
        "    df = df[['essay_id', 'essay', CONTROL_PANEL['target_col']]].dropna()\n",
        "\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=CONTROL_PANEL['random_state'])\n",
        "    print(f\"‚úÖ Data Prepared: {len(train_df)} Train, {len(test_df)} Test.\")\n",
        "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
        "\n",
        "def generate_embeddings(train_df, test_df):\n",
        "    print(\"üß† Generating Semantic Embeddings...\")\n",
        "    model = SentenceTransformer(CONTROL_PANEL['embedding_model'], device='cpu')\n",
        "    train_emb = model.encode(train_df['essay'].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
        "    test_emb = model.encode(test_df['essay'].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
        "    return train_emb, test_emb\n",
        "\n",
        "# --- Cell 4: Feature Engineering (Linguistic) ---\n",
        "def extract_linguistic_features(df):\n",
        "    print(\"   -> Extracting Linguistic Features (Readability, Counts)...\")\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. Length & Structure\n",
        "    df['char_count'] = df['essay'].apply(len)\n",
        "    df['word_count'] = df['essay'].apply(lambda x: len(str(x).split()))\n",
        "    df['sentence_count'] = df['essay'].apply(textstat.sentence_count)\n",
        "    df['avg_sentence_len'] = df['word_count'] / (df['sentence_count'] + 1)\n",
        "\n",
        "    # 2. Complexity / Readability\n",
        "    df['flesch_kincaid'] = df['essay'].apply(textstat.flesch_kincaid_grade)\n",
        "    df['gunning_fog'] = df['essay'].apply(textstat.gunning_fog)\n",
        "\n",
        "    # 3. Vocabulary Richness\n",
        "    def get_ttr(text):\n",
        "        words = str(text).lower().split()\n",
        "        if not words: return 0\n",
        "        return len(set(words)) / len(words)\n",
        "\n",
        "    df['ttr'] = df['essay'].apply(get_ttr)\n",
        "\n",
        "    feature_cols = ['char_count', 'word_count', 'sentence_count', 'avg_sentence_len', 'flesch_kincaid', 'gunning_fog', 'ttr']\n",
        "    return df[feature_cols].values\n",
        "\n",
        "# --- Cell 5: EXPERT 1 & 2 (Math Specialists - ENHANCED) ---\n",
        "def run_math_specialists(test_df, train_df, test_emb, train_emb):\n",
        "    print(\"\\n--- Running Math Specialists (RAG + Enhanced RF) ---\")\n",
        "\n",
        "    # 1. Numeric RAG\n",
        "    train_scores = train_df[CONTROL_PANEL['target_col']].values\n",
        "    index = faiss.IndexFlatL2(train_emb.shape[1])\n",
        "    index.add(train_emb)\n",
        "    D, I = index.search(test_emb, k=CONTROL_PANEL['top_k_numeric'])\n",
        "    test_df['feat_rag_numeric'] = [np.mean(train_scores[idx_list]) for idx_list in I]\n",
        "\n",
        "    # 2. Enhanced Random Forest\n",
        "    print(\"   -> Extracting Features & Training RF...\")\n",
        "    train_feats = extract_linguistic_features(train_df)\n",
        "    test_feats = extract_linguistic_features(test_df)\n",
        "\n",
        "    X_train = np.hstack([train_emb, train_feats])\n",
        "    X_test = np.hstack([test_emb, test_feats])\n",
        "\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, train_scores)\n",
        "    test_df['feat_rf'] = rf.predict(X_test)\n",
        "\n",
        "    return test_df, index\n",
        "\n",
        "# --- Cell 6: EXPERT 3 (DeepSeek via vLLM) ---\n",
        "\n",
        "def run_delta_llm_vllm(test_df, train_df, test_emb, index):\n",
        "    print(\"\\n--- Running Expert 3: DeepSeek Reasoning (vLLM Engine) ---\")\n",
        "\n",
        "    # 1. Initialize vLLM\n",
        "    from vllm import LLM, SamplingParams\n",
        "\n",
        "    print(f\"   -> Loading vLLM Engine: {CONTROL_PANEL['llm_model']}\")\n",
        "    llm = LLM(\n",
        "        model=CONTROL_PANEL['llm_model'],\n",
        "        quantization=\"awq\",\n",
        "        dtype=\"half\",\n",
        "        gpu_memory_utilization=CONTROL_PANEL['gpu_memory_utilization'],\n",
        "        max_model_len=8192,           # Increased max_len to support Set 8 context\n",
        "        trust_remote_code=True,\n",
        "        enforce_eager=True\n",
        "    )\n",
        "\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.0,\n",
        "        max_tokens=CONTROL_PANEL['max_new_tokens']\n",
        "    )\n",
        "\n",
        "    # 2. Checkpoint Logic\n",
        "    checkpoint_file = CONTROL_PANEL['checkpoint_path']\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        print(\"   -> Loading checkpoint...\")\n",
        "        saved_df = pd.read_csv(checkpoint_file)\n",
        "        test_df = test_df.merge(saved_df[['essay_id', 'feat_delta_llm']], on='essay_id', how='left', suffixes=('', '_saved'))\n",
        "        if 'feat_delta_llm_saved' in test_df.columns:\n",
        "            test_df['feat_delta_llm'] = test_df['feat_delta_llm'].fillna(test_df['feat_delta_llm_saved'])\n",
        "            test_df.drop(columns=['feat_delta_llm_saved'], inplace=True)\n",
        "\n",
        "    if 'feat_delta_llm' not in test_df.columns:\n",
        "        test_df['feat_delta_llm'] = np.nan\n",
        "\n",
        "    # 3. Identify Work\n",
        "    indices_to_process = [i for i in range(len(test_df)) if pd.isna(test_df.at[i, 'feat_delta_llm'])]\n",
        "    print(f\"   -> Processing {len(indices_to_process)} essays.\")\n",
        "\n",
        "    if not indices_to_process:\n",
        "        return test_df\n",
        "\n",
        "    D, I = index.search(test_emb, k=1)\n",
        "    limit = CONTROL_PANEL['context_char_limit']\n",
        "    batch_size = CONTROL_PANEL['vllm_batch_size']\n",
        "\n",
        "    # 4. Batch Processing Loop\n",
        "    pbar = tqdm(total=len(indices_to_process))\n",
        "\n",
        "    for i in range(0, len(indices_to_process), batch_size):\n",
        "        batch_idx = indices_to_process[i : i + batch_size]\n",
        "        prompts = []\n",
        "\n",
        "        # Prepare Batch Prompts\n",
        "        for idx in batch_idx:\n",
        "            ref_idx = I[idx][0]\n",
        "            ref_text = train_df.iloc[ref_idx]['essay']\n",
        "            ref_score = train_df.iloc[ref_idx][CONTROL_PANEL['target_col']]\n",
        "            student_text = test_df.at[idx, 'essay']\n",
        "\n",
        "            p = f\"\"\"<|user|>\n",
        "Reference Essay (Score: {ref_score}):\n",
        "\"{str(ref_text)[:limit]}\"\n",
        "\n",
        "Student Essay (Target):\n",
        "\"{str(student_text)[:limit]}\"\n",
        "\n",
        "Compare the Student Essay to the Reference Essay.\n",
        "Think step-by-step about the differences in quality.\n",
        "Is the Student Essay better, worse, or equal?\n",
        "Output the Final Score.\n",
        "\n",
        "Format:\n",
        "<think>\n",
        "... reasoning ...\n",
        "</think>\n",
        "JSON: {{\"score\": int}}\n",
        "<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "            prompts.append(p)\n",
        "\n",
        "        # Run vLLM Inference\n",
        "        try:\n",
        "            outputs = llm.generate(prompts, sampling_params, use_tqdm=False)\n",
        "\n",
        "            for j, output in enumerate(outputs):\n",
        "                original_idx = batch_idx[j]\n",
        "\n",
        "                # Parse\n",
        "                gen = output.outputs[0].text\n",
        "                if \"</think>\" in gen:\n",
        "                    gen = gen.split(\"</think>\")[-1]\n",
        "\n",
        "                try:\n",
        "                    data = repair_json(gen, return_objects=True)\n",
        "                    if isinstance(data, list): data = data[0]\n",
        "\n",
        "                    ref_idx = I[original_idx][0]\n",
        "                    ref_score = train_df.iloc[ref_idx][CONTROL_PANEL['target_col']]\n",
        "                    score = float(data.get('score', ref_score))\n",
        "                except:\n",
        "                    ref_idx = I[original_idx][0]\n",
        "                    score = float(train_df.iloc[ref_idx][CONTROL_PANEL['target_col']])\n",
        "\n",
        "                test_df.at[original_idx, 'feat_delta_llm'] = score\n",
        "\n",
        "            pbar.update(len(batch_idx))\n",
        "\n",
        "            # Checkpoint\n",
        "            if i % 10 == 0:\n",
        "                test_df.to_csv(checkpoint_file, index=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch: {e}\")\n",
        "            test_df.to_csv(checkpoint_file, index=False)\n",
        "\n",
        "    pbar.close()\n",
        "    test_df.to_csv(checkpoint_file, index=False)\n",
        "\n",
        "    # Cleanup vLLM\n",
        "    del llm\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return test_df\n",
        "\n",
        "# --- Cell 7: DUAL STACKING ---\n",
        "def run_dual_stacking(df):\n",
        "    print(\"\\n--- Final Dual Stacking ---\")\n",
        "    df['feat_delta_llm'] = df['feat_delta_llm'].fillna(df['feat_rf'])\n",
        "    X = df[['feat_rag_numeric', 'feat_rf', 'feat_delta_llm']]\n",
        "    y = df[CONTROL_PANEL['target_col']]\n",
        "\n",
        "    meta_lin = LinearRegression()\n",
        "    meta_lin.fit(X, y)\n",
        "    pred_lin = meta_lin.predict(X)\n",
        "\n",
        "    meta_rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
        "    meta_rf.fit(X, y)\n",
        "    pred_rf = meta_rf.predict(X)\n",
        "\n",
        "    df['pred_final'] = (pred_lin + pred_rf) / 2\n",
        "    return df, meta_lin.coef_\n",
        "\n",
        "# --- Cell 8: EXECUTION LOOP ---\n",
        "train_df, test_df = load_and_prep_data()\n",
        "\n",
        "if CONTROL_PANEL['pilot_sample_size']:\n",
        "    print(f\"‚ö†Ô∏è PILOT MODE: Sampling {CONTROL_PANEL['pilot_sample_size']} essays.\")\n",
        "    test_df = test_df.sample(n=CONTROL_PANEL['pilot_sample_size'], random_state=42).reset_index(drop=True)\n",
        "else:\n",
        "    print(\"üöÄ FULL RUN: Processing entire Test Set.\")\n",
        "\n",
        "train_emb, test_emb = generate_embeddings(train_df, test_df)\n",
        "\n",
        "# Specialists (Enhanced RF)\n",
        "test_df, faiss_index = run_math_specialists(test_df, train_df, test_emb, train_emb)\n",
        "\n",
        "# DeepSeek (vLLM)\n",
        "test_df = run_delta_llm_vllm(test_df, train_df, test_emb, faiss_index)\n",
        "\n",
        "# Stacking\n",
        "test_df, lin_weights = run_dual_stacking(test_df)\n",
        "\n",
        "# Evaluation\n",
        "y_true = test_df[CONTROL_PANEL['target_col']]\n",
        "metrics = []\n",
        "for col, name in [\n",
        "    ('feat_rag_numeric', '1. Numeric RAG'),\n",
        "    ('feat_rf', '2. Random Forest (Enhanced)'),\n",
        "    ('feat_delta_llm', '3. DeepSeek Delta'),\n",
        "    ('pred_final', '4. DUAL ENSEMBLE')\n",
        "]:\n",
        "    qwk = cohen_kappa_score(y_true, np.rint(test_df[col]), weights='quadratic')\n",
        "    mae = mean_absolute_error(y_true, test_df[col])\n",
        "    metrics.append({\"Model\": name, \"QWK\": qwk, \"MAE\": mae})\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"üèÜ FINAL RESULTS (SET {CONTROL_PANEL['essay_set']})\")\n",
        "print(\"=\"*60)\n",
        "print(metrics_df.to_string(index=False))\n",
        "print(\"-\" * 60)\n",
        "print(f\"Linear Weights: RAG={lin_weights[0]:.2f}, RF={lin_weights[1]:.2f}, DS={lin_weights[2]:.2f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=y_true, y=test_df['pred_final'], alpha=0.6, color='blue', label='Predictions')\n",
        "sns.regplot(x=y_true, y=test_df['pred_final'], scatter=False, color='red', label='Trend Line')\n",
        "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', label='Perfect Fit')\n",
        "plt.title(f\"Human vs AI Scores (QWK: {metrics_df.iloc[3]['QWK']:.4f})\")\n",
        "plt.xlabel(\"Human Score\")\n",
        "plt.ylabel(\"AI Prediction\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig(\"thesis_correlation_plot.png\")\n",
        "plt.show()\n",
        "\n",
        "# Download\n",
        "print(\"Downloading Final Results...\")\n",
        "test_df.to_csv(\"FINAL_THESIS_RESULTS.csv\", index=False)\n",
        "files.download(\"FINAL_THESIS_RESULTS.csv\")\n",
        "files.download(\"thesis_correlation_plot.png\")"
      ]
    }
  ]
}